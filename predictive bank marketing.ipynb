{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.6.3","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12033,"sourceType":"datasetVersion","datasetId":8599}],"dockerImageVersionId":33,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predictive analysis of Bank Marketing\n\n#### Problem Statement\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n\n#### What to achieve?\nThe classification goal is to predict if the client will subscribe a term deposit (variable y).\n\n#### Data Contains information in following format:\n\n### Categorical Variable :\n\n* Marital - (Married , Single , Divorced)\",\n* Job - (Management,BlueCollar,Technician,entrepreneur,retired,admin.,services,selfemployed,housemaid,student,unemployed,unknown)\n* Contact - (Telephone,Cellular,Unknown)\n* Education - (Primary,Secondary,Tertiary,Unknown)\n* Month - (Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec)\n* Poutcome - (Success,Failure,Other,Unknown)\n* Housing - (Yes/No)\n* Loan - (Yes/No)\n* Default - (Yes/No)\n\n### Numerical Variable:\n\n* Age\n* Balance\n* Day\n* Duration\n* Campaign\n* Pdays\n* Previous\n\n#### Class\n* deposit - (Yes/No)","metadata":{"_uuid":"c4c944fdc7d7377793e9069466f927115b0759d3","_cell_guid":"39eb6eaf-f830-4c9f-9ad1-8d13ff08c0ac"}},{"cell_type":"markdown","source":"1. **Importing required libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nimport statsmodels.formula.api as smf\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n\n#Classification Algorithms \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics as m\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing and displaying the data**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/bank.csv\", delimiter=\";\",header='infer')\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding correlation between features and class for selection**","metadata":{}},{"cell_type":"markdown","source":"**1. Using Pairplot**","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that data here is not-symmetric. So lets find out the correlation matrix to look into details.","metadata":{}},{"cell_type":"markdown","source":"**2. Correlation Matrix**","metadata":{}},{"cell_type":"code","source":"data.corr()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Heatplot to visualise correlation**","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As per the pairplot, correlation matrix, and heatmap, observations as follow:\n* Data is non-linear, asymmetric\n* Hence selection of features will not depend upon correlation factor.\n* Also not a single feature is correlated completely with class, hence requires combinantion of features.","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection techniques:\n1. Univariate Selection (non-negative features)\n2. Recursive Feature Elimination (RFE)\n3. Principal Component Analysis (PCA) (data reduction technique)\n4. Feature Importance (decision trees)\n\n#### Which feature selection technique should be used for our data?\n* Contains negative values, hence Univariate Selection technique cannot be used.\n* PCA is data reduction technique. Aim is to select best possible feature and not reduction and this is classification type of data. \n* PCA is an unsupervised method, used for dimensionality reduction.\n* Hence Decision tree technique and RFE can be used for feature selection.\n* Best possible technique will be which gives extracts columns who provide better accuracy.","metadata":{}},{"cell_type":"markdown","source":"**Encoding Categorical and numerical data into digits form.**","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting object type data into One-Hot Encoded data using get_dummies method.","metadata":{}},{"cell_type":"code","source":"data_new = pd.get_dummies(data, columns=['job','marital',\n                                         'education','default',\n                                         'housing','loan',\n                                         'contact','month',\n                                         'poutcome'])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Class column into binary format\ndata_new.y.replace(('yes', 'no'), (1, 0), inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Successfully converted data into  integer data types\ndata_new.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring features: Age as a example**","metadata":{}},{"cell_type":"code","source":"#Whole dataset's shape (ie (rows, cols))\nprint(data.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unique education values\ndata.education.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Crosstab to display education stats with respect to y ie class variable\npd.crosstab(index=data[\"education\"], columns=data[\"y\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Education categories and there frequency\ndata.education.value_counts().plot(kind=\"barh\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classifiers : Based on the values of different parameters we can conclude to the following classifiers for Binary Classification.\n\n    1. Gradient Boosting\n    2. AdaBoosting\n    3. Logistics Regression\n    4. Random Forest Classifier\n    5. Linear Discriminant Analysis\n    6. K Nearest Neighbour\n    7. Decision Tree\n    8. Gaussian Naive Bayes \n    9. Support Vector Classifier\n\n#### And performance metric using precision and recall calculation along with roc_auc_score & accuracy_score","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nclassifiers = {\n               'Adaptive Boosting Classifier':AdaBoostClassifier(),\n               'Linear Discriminant Analysis':LinearDiscriminantAnalysis(),\n               'Logistic Regression':LogisticRegression(),\n               'Random Forest Classifier': RandomForestClassifier(),\n               'K Nearest Neighbour':KNeighborsClassifier(8),\n               'Decision Tree Classifier':DecisionTreeClassifier(),\n               'Gaussian Naive Bayes Classifier':GaussianNB(),\n               'Support Vector Classifier':SVC(),\n               }","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Due to one hot encoding increase in the number of columns\ndata_new.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_y = pd.DataFrame(data_new['y'])\ndata_X = data_new.drop(['y'], axis=1)\nprint(data_X.columns)\nprint(data_y.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_cols = [\"Classifier\", \"Accuracy\",\"Precision Score\",\"Recall Score\",\"F1-Score\",\"roc-auc_Score\"]\nlog = pd.DataFrame(columns=log_cols)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nrs = StratifiedShuffleSplit(n_splits=2, test_size=0.3,random_state=2)\nrs.get_n_splits(data_X,data_y)\nfor Name,classify in classifiers.items():\n    for train_index, test_index in rs.split(data_X,data_y):\n        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X,X_test = data_X.iloc[train_index], data_X.iloc[test_index]\n        y,y_test = data_y.iloc[train_index], data_y.iloc[test_index]\n        # Scaling of Features \n#         from sklearn.preprocessing import StandardScaler\n#         sc_X = StandardScaler()\n#         X = sc_X.fit_transform(X)\n#         X_test = sc_X.transform(X_test)\n        cls = classify\n        cls =cls.fit(X,y)\n        y_out = cls.predict(X_test)\n        accuracy = m.accuracy_score(y_test,y_out)\n        precision = m.precision_score(y_test,y_out,average='macro')\n        recall = m.recall_score(y_test,y_out,average='macro')\n        #roc_auc = roc_auc_score(y_out,y_test)\n        f1_score = m.f1_score(y_test,y_out,average='macro')\n        log_entry = pd.DataFrame([[Name,accuracy,precision,recall,f1_score,roc_auc]], columns=log_cols)\n        #metric_entry = pd.DataFrame([[precision,recall,f1_score,roc_auc]], columns=metrics_cols)\n        log = log.append(log_entry)\n        #metric = metric.append(metric_entry)\n        \nprint(log)\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"g\")  \nplt.show()\n\n#Scroll complete output to view all the accuracy scores and bar graph.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see highest accuracy for Logistic Regression.\n\n### Why logistic regression?\n\nThe models are equivalent in terms of the functions they can express, so with infinite training data and a function where the input variables don't interact with each other in any way they will both probably asymptotically approach the underlying joint probability distribution. This would definitely not be true if your features were not all binary.\n\nGradient boosted stumps adds extra machinery that sounds like it is irrelevant to your task. Logistic regression will efficiently compute a maximum likelihood estimate assuming that all the inputs are independent. I would go with logistic regression.\n","metadata":{}},{"cell_type":"markdown","source":"**For independent Execution of Logistic Regression, Code as follows:**","metadata":{}},{"cell_type":"code","source":"#Divide records in training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.3, random_state=2, stratify=data_y)\nprint (X_train.shape)\nprint (X_test.shape)\nprint (y_train.shape)\nprint (y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create an Logistic classifier and train it on 70% of the data set.\nfrom sklearn import svm\nfrom xgboost import XGBClassifier\nclf = LogisticRegression()\nclf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fiting into model\nclf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction using test data\ny_pred = clf.predict(X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#classification accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predictions\npredictions = clf.predict(X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\nprint(confusion_matrix(y_test, predictions))\n\n# New line\nprint('\\n')\n\n# Classification report\nprint(classification_report(y_test,predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:\n### Used the following:\n* Feature Selection - RFE-LogisticRegression\n* Fiting - SVM\n* With 0.8938 Accuracy (0.3% Test data)****","metadata":{}},{"cell_type":"code","source":"","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}